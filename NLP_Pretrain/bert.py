# ComfyNN NLP Pretrain BERT Nodes
# Based on d2l-zh implementation (https://github.com/d2l-ai/d2l-zh)
# Thank you d2l-ai team for the excellent educational resource

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math

class BERTEncoder(nn.Module):
    """BERTç¼–ç å™¨"""
    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,
                 ffn_num_hiddens, num_heads, num_layers, dropout,
                 max_len=1000, key_size=768, query_size=768, value_size=768,
                 **kwargs):
        super(BERTEncoder, self).__init__(**kwargs)
        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)
        self.segment_embedding = nn.Embedding(2, num_hiddens)
        self.blks = nn.Sequential()
        for i in range(num_layers):
            self.blks.add_module(f"{i}", self.EncoderBlock(
                key_size, query_size, value_size, num_hiddens, norm_shape,
                ffn_num_input, ffn_num_hiddens, num_heads, dropout, True))
        # åœ¨BERTä¸­ï¼Œä½ç½®åµŒå…¥æ˜¯å¯å­¦ä¹ çš„ï¼Œå› æ­¤æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªè¶³å¤Ÿé•¿çš„ä½ç½®åµŒå…¥å‚æ•°
        self.pos_embedding = nn.Parameter(torch.randn(1, max_len, num_hiddens))

    def forward(self, tokens, segments, valid_lens):
        # åœ¨ä»¥ä¸‹ä»£ç æ®µä¸­ï¼ŒXçš„å½¢çŠ¶ä¿æŒä¸å˜ï¼šï¼ˆæ‰¹é‡å¤§å°ï¼Œæœ€å¤§åºåˆ—é•¿åº¦ï¼Œnum_hiddensï¼‰
        X = self.token_embedding(tokens) + self.segment_embedding(segments)
        X = X + self.pos_embedding.data[:, :X.shape[1], :]
        for blk in self.blks:
            X = blk(X, valid_lens)
        return X

    class EncoderBlock(nn.Module):
        """Transformerç¼–ç å™¨å—"""
        def __init__(self, key_size, query_size, value_size, num_hiddens,
                     norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,
                     dropout, use_bias=False, **kwargs):
            super().__init__(**kwargs)
            self.attention = self.MultiHeadAttention(
                key_size, query_size, value_size, num_hiddens, num_heads, dropout, use_bias)
            self.addnorm1 = self.AddNorm(norm_shape, dropout)
            self.ffn = self.PositionWiseFFN(ffn_num_input, ffn_num_hiddens, num_hiddens)
            self.addnorm2 = self.AddNorm(norm_shape, dropout)

        def forward(self, X, valid_lens):
            Y, _ = self.attention(X, X, X, valid_lens)
            X = self.addnorm1(X, Y)
            Y = self.ffn(X)
            X = self.addnorm2(X, Y)
            return X

        class MultiHeadAttention(nn.Module):
            """å¤šå¤´æ³¨æ„åŠ›"""
            def __init__(self, key_size, query_size, value_size, num_hiddens,
                         num_heads, dropout, bias=False, **kwargs):
                super().__init__(**kwargs)
                self.num_heads = num_heads
                self.attention = self.DotProductAttention(dropout)
                self.W_q = nn.Linear(query_size, num_hiddens, bias=bias)
                self.W_k = nn.Linear(key_size, num_hiddens, bias=bias)
                self.W_v = nn.Linear(value_size, num_hiddens, bias=bias)
                self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)

            def forward(self, queries, keys, values, valid_lens):
                queries = self.W_q(queries)
                keys = self.W_k(keys)
                values = self.W_v(values)
                
                queries = self.transpose_qkv(queries)
                keys = self.transpose_qkv(keys)
                values = self.transpose_qkv(values)
                
                if valid_lens is not None:
                    valid_lens = torch.repeat_interleave(valid_lens, repeats=self.num_heads, dim=0)
                
                output, attention_weights = self.attention(queries, keys, values, valid_lens)
                output = self.transpose_output(output)
                return self.W_o(output), attention_weights

            def transpose_qkv(self, X):
                """å˜æ¢å½¢çŠ¶ä»¥å®ç°å¤šå¤´æ³¨æ„åŠ›"""
                X = X.reshape(X.shape[0], X.shape[1], self.num_heads, -1)
                X = X.permute(0, 2, 1, 3)
                return X.reshape(-1, X.shape[2], X.shape[3])

            def transpose_output(self, X):
                """è¿˜åŸå½¢çŠ¶"""
                X = X.reshape(-1, self.num_heads, X.shape[1], X.shape[2])
                X = X.permute(0, 2, 1, 3)
                return X.reshape(X.shape[0], X.shape[1], -1)

            class DotProductAttention(nn.Module):
                """ç‚¹ç§¯æ³¨æ„åŠ›"""
                def __init__(self, dropout, **kwargs):
                    super().__init__(**kwargs)
                    self.dropout = nn.Dropout(dropout)

                def forward(self, queries, keys, values, valid_lens=None):
                    d = queries.shape[-1]
                    scores = torch.bmm(queries, keys.transpose(1,2)) / math.sqrt(d)
                    
                    if valid_lens is not None:
                        scores = self.mask_softmax(scores, valid_lens)
                    else:
                        self.attention_weights = F.softmax(scores, dim=-1)
                    
                    return torch.bmm(self.dropout(self.attention_weights), values), self.attention_weights

                def mask_softmax(self, X, valid_lens):
                    """é®è”½softmax"""
                    if valid_lens is None:
                        return F.softmax(X, dim=-1)
                    else:
                        shape = X.shape
                        if valid_lens.dim() == 1:
                            valid_lens = torch.repeat_interleave(valid_lens, shape[1])
                        else:
                            valid_lens = valid_lens.reshape(-1)
                        X = X.reshape((-1, shape[-1]))
                        max_len = X.shape[1]
                        batch_size = X.shape[0]
                        valid_lens = valid_lens.reshape((batch_size, 1))
                        mask = torch.arange(max_len, dtype=torch.float32, device=X.device).reshape((1, max_len)).expand(batch_size, max_len) >= valid_lens
                        X = X.masked_fill_(mask, -1e6)
                        self.attention_weights = F.softmax(X.reshape(shape), dim=-1)
                        return self.attention_weights

        class AddNorm(nn.Module):
            """æ®‹å·®è¿æ¥åè¿›è¡Œå±‚è§„èŒƒåŒ–"""
            def __init__(self, normalized_shape, dropout, **kwargs):
                super().__init__(**kwargs)
                self.dropout = nn.Dropout(dropout)
                self.ln = nn.LayerNorm(normalized_shape)

            def forward(self, X, Y):
                return self.ln(X + self.dropout(Y))

        class PositionWiseFFN(nn.Module):
            """åŸºäºä½ç½®çš„å‰é¦ˆç½‘ç»œ"""
            def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs, **kwargs):
                super().__init__(**kwargs)
                self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)
                self.relu = nn.ReLU()
                self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)

            def forward(self, X):
                return self.dense2(self.relu(self.dense1(X)))


def get_tokens_and_segments(tokens_a, tokens_b=None):
    """è·å–è¾“å…¥åºåˆ—çš„è¯å…ƒåŠå…¶ç‰‡æ®µç´¢å¼•"""
    tokens = ['<cls>'] + tokens_a + ['<sep>']
    # 0å’Œ1åˆ†åˆ«æ ‡è®°ç‰‡æ®µAå’ŒB
    segments = [0] * (len(tokens_a) + 2)
    if tokens_b is not None:
        tokens += tokens_b + ['<sep>']
        segments += [1] * (len(tokens_b) + 1)
    return tokens, segments


class BERTModel:
    """BERTæ¨¡å‹èŠ‚ç‚¹"""
    
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "vocab_size": ("INT", {"default": 10000, "min": 1000, "max": 100000}),
                "num_hiddens": ("INT", {"default": 768, "min": 128, "max": 2048}),
                "ffn_num_hiddens": ("INT", {"default": 1024, "min": 256, "max": 4096}),
                "num_heads": ("INT", {"default": 4, "min": 1, "max": 16}),
                "num_layers": ("INT", {"default": 2, "min": 1, "max": 24}),
                "dropout": ("FLOAT", {"default": 0.2, "min": 0.0, "max": 0.9, "step": 0.05}),
            },
            "optional": {
                "max_len": ("INT", {"default": 1000, "min": 100, "max": 5000}),
            }
        }

    RETURN_TYPES = ("CUSTOM",)
    RETURN_NAMES = ("bert_model",)
    FUNCTION = "create_bert"
    CATEGORY = "ComfyNN/NLP_Pretrain/BERT"
    DESCRIPTION = "Create BERT model based on d2l-zh implementation"

    def create_bert(self, vocab_size, num_hiddens, ffn_num_hiddens, num_heads, 
                    num_layers, dropout, max_len=1000):
        norm_shape = [num_hiddens]
        ffn_num_input = num_hiddens
        
        bert_encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape, ffn_num_input,
                                   ffn_num_hiddens, num_heads, num_layers, dropout, max_len)
        
        return (bert_encoder,)


class BERTMaskedLanguageModel:
    """BERTæ©ç è¯­è¨€æ¨¡å‹èŠ‚ç‚¹"""
    
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "bert_model": ("CUSTOM",),
                "mlm_weights": ("TENSOR",),
                "mlm_bias": ("TENSOR",),
                "tokens": ("STRING", {"multiline": True, "default": "this is a masked language model example"}),
                "masked_positions": ("STRING", {"default": "1,3,5"}),  # ä»¥é€—å·åˆ†éš”çš„ä½ç½®
            }
        }

    RETURN_TYPES = ("TENSOR", "STRING")
    RETURN_NAMES = ("predictions", "mlm_info")
    FUNCTION = "predict_masked_tokens"
    CATEGORY = "ComfyNN/NLP_Pretrain/BERT"
    DESCRIPTION = "Predict masked tokens using BERT model"

    def predict_masked_tokens(self, bert_model, mlm_weights, mlm_bias, tokens, masked_positions):
        # è§£æè¾“å…¥
        token_list = tokens.strip().split()
        masked_positions_list = [int(pos) for pos in masked_positions.split(",") if pos.strip()]
        
        # æ„å»ºè¾“å…¥
        input_tokens, segments = get_tokens_and_segments(token_list)
        
        # è½¬æ¢ä¸ºç´¢å¼•ï¼ˆç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥ä½¿ç”¨è¯æ±‡è¡¨ï¼‰
        # è¿™é‡Œæˆ‘ä»¬åªæ˜¯æ¼”ç¤ºç»“æ„ï¼Œå®é™…å®ç°éœ€è¦å®Œæ•´çš„è¯æ±‡è¡¨å¤„ç†
        vocab_size = bert_model.token_embedding.num_embeddings
        token_indices = torch.randint(0, vocab_size, (1, len(input_tokens)))
        segment_indices = torch.tensor([segments])
        valid_lens = torch.tensor([len(input_tokens)])
        
        # BERTç¼–ç 
        encoded_X = bert_model(token_indices, segment_indices, valid_lens)
        
        # è·å–é®è”½ä½ç½®çš„è¯å…ƒè¡¨ç¤º
        masked_positions_tensor = torch.tensor(masked_positions_list)
        masked_X = encoded_X[:, masked_positions_tensor, :]
        
        # é€šè¿‡MLMå¤´é¢„æµ‹
        predictions = torch.matmul(masked_X, mlm_weights.transpose(1, 0)) + mlm_bias
        
        # ç”Ÿæˆä¿¡æ¯
        mlm_info = f"BERT MLM Prediction\n"
        mlm_info += f"Input tokens: {len(input_tokens)}\n"
        mlm_info += f"Masked positions: {len(masked_positions_list)}\n"
        mlm_info += f"Vocabulary size: {vocab_size}\n"
        mlm_info += f"Predictions shape: {list(predictions.shape)}"
        
        return (predictions, mlm_info)

# Node mappings
NODE_CLASS_MAPPINGS = {
    "BERTModel": BERTModel,
    "BERTMaskedLanguageModel": BERTMaskedLanguageModel,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "BERTModel": "BERT Model ğŸ±",
    "BERTMaskedLanguageModel": "BERT Masked Language Model ğŸ±",
}
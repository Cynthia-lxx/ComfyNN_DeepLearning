# ComfyNN ComputerVision Bounding Box Nodes
import torch
import numpy as np

class BoundingBoxNode:
    """è¾¹ç•Œæ¡†ç”ŸæˆèŠ‚ç‚¹"""
    
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image_batch": ("IMAGE",),
                "bbox_format": (["xyxy", "xywh", "cxcywh"], {"default": "xyxy"}),
            },
            "optional": {
                "normalize_coords": ("BOOLEAN", {"default": False}),
                "add_padding": ("BOOLEAN", {"default": False}),
                "padding_ratio": ("FLOAT", {"default": 0.1, "min": 0.0, "max": 0.5, "step": 0.01}),
            }
        }

    RETURN_TYPES = ("TENSOR", "STRING")
    RETURN_NAMES = ("bounding_boxes", "bbox_info")
    FUNCTION = "generate"
    CATEGORY = "ComfyNN/ComputerVision/BoundingBox"
    DESCRIPTION = "ç”Ÿæˆè¾¹ç•Œæ¡†"

    def generate(self, image_batch, bbox_format, normalize_coords=False, add_padding=False, padding_ratio=0.1):
        # è·å–å›¾åƒæ‰¹æ¬¡ä¿¡æ¯
        batch_size, height, width = image_batch.shape[:3]
        
        # ç”Ÿæˆç¤ºä¾‹è¾¹ç•Œæ¡†ï¼ˆåœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™äº›åº”è¯¥æ¥è‡ªç›®æ ‡æ£€æµ‹ç®—æ³•ï¼‰
        # è¿™é‡Œæˆ‘ä»¬ç”Ÿæˆä¸€äº›éšæœºè¾¹ç•Œæ¡†ä½œä¸ºç¤ºä¾‹
        bboxes = []
        for i in range(batch_size):
            # ç”Ÿæˆä¸€ä¸ªéšæœºè¾¹ç•Œæ¡†
            x1 = np.random.randint(0, width // 2)
            y1 = np.random.randint(0, height // 2)
            x2 = np.random.randint(width // 2, width)
            y2 = np.random.randint(height // 2, height)
            
            if bbox_format == "xyxy":
                bbox = [x1, y1, x2, y2]
            elif bbox_format == "xywh":
                bbox = [x1, y1, x2 - x1, y2 - y1]
            elif bbox_format == "cxcywh":
                cx = (x1 + x2) / 2
                cy = (y1 + y2) / 2
                w = x2 - x1
                h = y2 - y1
                bbox = [cx, cy, w, h]
            
            bboxes.append(bbox)
        
        # è½¬æ¢ä¸ºtensor
        bbox_tensor = torch.tensor(bboxes, dtype=torch.float32)
        
        # å¦‚æœéœ€è¦å½’ä¸€åŒ–åæ ‡
        if normalize_coords:
            if bbox_format == "xyxy":
                bbox_tensor[:, [0, 2]] /= width
                bbox_tensor[:, [1, 3]] /= height
            elif bbox_format == "xywh":
                bbox_tensor[:, [0, 2]] /= width
                bbox_tensor[:, [1, 3]] /= height
            elif bbox_format == "cxcywh":
                bbox_tensor[:, [0, 2]] /= width
                bbox_tensor[:, [1, 3]] /= height
        
        # ç”Ÿæˆè¾¹ç•Œæ¡†ä¿¡æ¯å­—ç¬¦ä¸²
        bbox_info = f"Batch size: {batch_size}\n"
        bbox_info += f"Image size: {width}x{height}\n"
        bbox_info += f"Bounding box format: {bbox_format}\n"
        bbox_info += f"Normalized coordinates: {normalize_coords}\n"
        bbox_info += f"Sample bbox: {bboxes[0] if bboxes else 'None'}"
        
        return (bbox_tensor, bbox_info)


class BoundingBoxMatchingNode:
    """è¾¹ç•Œæ¡†å¤„ç†èŠ‚ç‚¹"""
    
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "bounding_boxes": ("TENSOR",),
                "operation": (["scale", "shift", "clip", "filter"], {"default": "scale"}),
                "image_width": ("INT", {"default": 224, "min": 32, "max": 2048}),
                "image_height": ("INT", {"default": 224, "min": 32, "max": 2048}),
            },
            "optional": {
                "scale_factor": ("FLOAT", {"default": 1.0, "min": 0.1, "max": 3.0, "step": 0.1}),
                "shift_x": ("INT", {"default": 0, "min": -100, "max": 100}),
                "shift_y": ("INT", {"default": 0, "min": -100, "max": 100}),
                "min_area": ("INT", {"default": 100, "min": 1, "max": 10000}),
                "max_area": ("INT", {"default": 10000, "min": 100, "max": 100000}),
            }
        }

    RETURN_TYPES = ("TENSOR", "STRING")
    RETURN_NAMES = ("processed_bboxes", "processing_info")
    FUNCTION = "process"
    CATEGORY = "ComfyNN/ComputerVision/BoundingBox"
    DESCRIPTION = "å¤„ç†è¾¹ç•Œæ¡†"

    def process(self, bounding_boxes, operation, image_width, image_height, scale_factor=1.0,
                shift_x=0, shift_y=0, min_area=100, max_area=10000):
        # ç¡®ä¿è¾“å…¥æ˜¯torch.Tensor
        if not isinstance(bounding_boxes, torch.Tensor):
            raise TypeError("è¾“å…¥å¿…é¡»æ˜¯torch.Tensorç±»å‹")
        
        # å¤åˆ¶è¾¹ç•Œæ¡†ä»¥é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
        processed_bboxes = bounding_boxes.clone()
        
        # è·å–è¾¹ç•Œæ¡†æ ¼å¼ï¼ˆå‡è®¾æ˜¯xyxyæ ¼å¼ï¼‰
        if processed_bboxes.shape[1] < 4:
            raise ValueError("è¾¹ç•Œæ¡†å¿…é¡»è‡³å°‘åŒ…å«4ä¸ªåæ ‡å€¼")
        
        info = f"Operation: {operation}\n"
        
        if operation == "scale":
            # ç¼©æ”¾è¾¹ç•Œæ¡†
            processed_bboxes[:, [0, 2]] *= scale_factor  # x coordinates
            processed_bboxes[:, [1, 3]] *= scale_factor  # y coordinates
            info += f"Scaled by factor: {scale_factor}\n"
            
        elif operation == "shift":
            # å¹³ç§»è¾¹ç•Œæ¡†
            processed_bboxes[:, [0, 2]] += shift_x  # x coordinates
            processed_bboxes[:, [1, 3]] += shift_y  # y coordinates
            info += f"Shifted by ({shift_x}, {shift_y})\n"
            
        elif operation == "clip":
            # è£å‰ªè¾¹ç•Œæ¡†åˆ°å›¾åƒè¾¹ç•Œ
            processed_bboxes[:, [0, 2]] = torch.clamp(processed_bboxes[:, [0, 2]], 0, image_width)
            processed_bboxes[:, [1, 3]] = torch.clamp(processed_bboxes[:, [1, 3]], 0, image_height)
            info += f"Clipped to image size: {image_width}x{image_height}\n"
            
        elif operation == "filter":
            # æ ¹æ®é¢ç§¯è¿‡æ»¤è¾¹ç•Œæ¡†
            widths = processed_bboxes[:, 2] - processed_bboxes[:, 0]
            heights = processed_bboxes[:, 3] - processed_bboxes[:, 1]
            areas = widths * heights
            
            # ä¿ç•™é¢ç§¯åœ¨æŒ‡å®šèŒƒå›´å†…çš„è¾¹ç•Œæ¡†
            valid_indices = (areas >= min_area) & (areas <= max_area)
            processed_bboxes = processed_bboxes[valid_indices]
            info += f"Filtered by area [{min_area}, {max_area}]\n"
            info += f"Remaining boxes: {processed_bboxes.shape[0]}/{bounding_boxes.shape[0]}\n"
        
        info += f"Processed boxes shape: {processed_bboxes.shape}"
        
        return (processed_bboxes, info)

# Node mappings
NODE_CLASS_MAPPINGS = {
    "BoundingBoxNode": BoundingBoxNode,
    "BoundingBoxMatchingNode": BoundingBoxMatchingNode,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "BoundingBoxNode": "Bounding Box ğŸ±",
    "BoundingBoxMatchingNode": "Bounding Box Matching ğŸ±",
}
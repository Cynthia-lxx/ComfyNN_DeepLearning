import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Tuple, Dict, Any
from PIL import Image
import torchvision.models as models
import torchvision.transforms as transforms

class StyleTransferNode:
    """
    é£æ ¼è¿ç§»èŠ‚ç‚¹
    
    å®ç°ç¥ç»é£æ ¼è¿ç§»ç®—æ³•ï¼Œå°†å†…å®¹å›¾åƒçš„ç»“æ„ä¸é£æ ¼å›¾åƒçš„è‰ºæœ¯é£æ ¼ç›¸ç»“åˆã€‚
    æ”¯æŒå¤šç§é£æ ¼è¿ç§»ç®—æ³•ï¼ŒåŒ…æ‹¬åŸºäºä¼˜åŒ–çš„æ–¹æ³•å’Œå¿«é€Ÿé£æ ¼è¿ç§»ã€‚
    """
    
    @classmethod
    def INPUT_TYPES(cls) -> Dict[str, Dict[str, Any]]:
        return {
            "required": {
                "content_image": ("IMAGE",),
                "style_image": ("IMAGE",),
                "transfer_method": (["gatys_et_al", "johnson", "adain"], {"default": "adain"}),
                "content_weight": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 10.0, "step": 0.1}),
                "style_weight": ("FLOAT", {"default": 10.0, "min": 0.0, "max": 100.0, "step": 1.0})
            },
            "optional": {
                "num_iterations": ("INT", {"default": 300, "min": 10, "max": 1000, "step": 10}),
                "learning_rate": ("FLOAT", {"default": 0.01, "min": 0.0001, "max": 0.1, "step": 0.001}),
                "preserve_colors": ("BOOLEAN", {"default": False}),
                "style_layers": (["shallow", "deep", "mixed"], {"default": "mixed"}),
                "content_layers": (["shallow", "deep"], {"default": "deep"})
            }
        }

    RETURN_TYPES = ("IMAGE", "INT")
    RETURN_NAMES = ("stylized_image", "iterations_used")
    FUNCTION = "transfer_style"
    CATEGORY = "ComfyNN/DeepLearning/ComputerVision"

    def transfer_style(
        self,
        content_image: torch.Tensor,
        style_image: torch.Tensor,
        transfer_method: str,
        content_weight: float,
        style_weight: float,
        num_iterations: int = 300,
        learning_rate: float = 0.01,
        preserve_colors: bool = False,
        style_layers: str = "mixed",
        content_layers: str = "deep"
    ) -> Tuple[torch.Tensor, int]:
        """
        æ‰§è¡Œé£æ ¼è¿ç§»
        
        Args:
            content_image: å†…å®¹å›¾åƒ [B, H, W, C]
            style_image: é£æ ¼å›¾åƒ [B, H, W, C]
            transfer_method: è¿ç§»æ–¹æ³•
            content_weight: å†…å®¹æŸå¤±æƒé‡
            style_weight: é£æ ¼æŸå¤±æƒé‡
            num_iterations: è¿­ä»£æ¬¡æ•°
            learning_rate: å­¦ä¹ ç‡
            preserve_colors: æ˜¯å¦ä¿ç•™å†…å®¹å›¾åƒçš„é¢œè‰²
            style_layers: é£æ ¼å±‚é€‰æ‹©
            content_layers: å†…å®¹å±‚é€‰æ‹©
            
        Returns:
            stylized_image: é£æ ¼åŒ–å›¾åƒ [B, H, W, C]
            iterations_used: å®é™…ä½¿ç”¨çš„è¿­ä»£æ¬¡æ•°
        """
        # ç¡®ä¿è¾“å…¥å›¾åƒä¸ºæ­£ç¡®çš„æ ¼å¼
        batch_size, height, width, channels = content_image.shape
        
        # æ£€æŸ¥é£æ ¼å›¾åƒæ˜¯å¦ä¸å†…å®¹å›¾åƒå…·æœ‰ç›¸åŒçš„å°ºå¯¸
        if style_image.shape[1:3] != (height, width):
            # è°ƒæ•´é£æ ¼å›¾åƒå°ºå¯¸ä»¥åŒ¹é…å†…å®¹å›¾åƒ
            style_image = F.interpolate(
                style_image.permute(0, 3, 1, 2),
                size=(height, width),
                mode='bilinear',
                align_corners=False
            ).permute(0, 2, 3, 1)
        
        # æ ¹æ®è¿ç§»æ–¹æ³•æ‰§è¡Œç›¸åº”çš„é£æ ¼è¿ç§»
        if transfer_method == "gatys_et_al":
            # Gatysç­‰äººçš„åŸå§‹é£æ ¼è¿ç§»ç®—æ³•ï¼ˆåŸºäºä¼˜åŒ–ï¼‰
            stylized_image = self._gatys_style_transfer(
                content_image, style_image, num_iterations, learning_rate,
                content_weight, style_weight, style_layers, content_layers
            )
        elif transfer_method == "johnson":
            # Johnsonå¿«é€Ÿé£æ ¼è¿ç§»
            stylized_image = self._johnson_style_transfer(content_image, style_image)
        elif transfer_method == "adain":
            # AdaINé£æ ¼è¿ç§»
            stylized_image = self._adain_style_transfer(content_image, style_image, preserve_colors)
        else:
            # é»˜è®¤ä½¿ç”¨AdaINæ–¹æ³•
            stylized_image = self._adain_style_transfer(content_image, style_image, preserve_colors)
        
        # è¿”å›é£æ ¼åŒ–å›¾åƒå’Œè¿­ä»£æ¬¡æ•°
        return (stylized_image, num_iterations)

    def _gatys_style_transfer(
        self, 
        content_image: torch.Tensor, 
        style_image: torch.Tensor,
        num_iterations: int,
        learning_rate: float,
        content_weight: float,
        style_weight: float,
        style_layers: str,
        content_layers: str
    ) -> torch.Tensor:
        """
        Gatysç­‰äººçš„é£æ ¼è¿ç§»å®ç°ï¼ˆç®€åŒ–ç‰ˆï¼‰
        """
        # åˆå§‹åŒ–ç”Ÿæˆå›¾åƒä¸ºå†…å®¹å›¾åƒ
        generated_image = content_image.clone().requires_grad_(True)
        
        # åŠ è½½VGGæ¨¡å‹ç”¨äºç‰¹å¾æå–
        vgg = models.vgg19(pretrained=True).features.eval()
        
        # å®šä¹‰å†…å®¹å’Œé£æ ¼å±‚
        content_layers_default = ['conv_4']
        style_layers_default = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']
        
        if content_layers == "shallow":
            content_layers_list = ['conv_1']
        else:
            content_layers_list = content_layers_default
            
        if style_layers == "shallow":
            style_layers_list = ['conv_1', 'conv_2']
        elif style_layers == "deep":
            style_layers_list = ['conv_3', 'conv_4', 'conv_5']
        else:
            style_layers_list = style_layers_default
        
        # æå–å†…å®¹å’Œé£æ ¼ç‰¹å¾ï¼ˆç®€åŒ–å®ç°ï¼‰
        # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šé€šè¿‡VGGç½‘ç»œæå–ç‰¹å¾
        
        # æ¨¡æ‹Ÿä¼˜åŒ–è¿‡ç¨‹
        optimizer = torch.optim.Adam([generated_image], lr=learning_rate)
        
        for i in range(min(num_iterations, 10)):  # é™åˆ¶è¿­ä»£æ¬¡æ•°ä»¥é¿å…é•¿æ—¶é—´è¿è¡Œ
            # è®¡ç®—æŸå¤±ï¼ˆç®€åŒ–ï¼‰
            content_loss = F.mse_loss(generated_image, content_image)
            style_loss = F.mse_loss(
                generated_image.mean(dim=[1, 2]), 
                style_image.mean(dim=[1, 2])
            )
            
            # æ€»æŸå¤±
            total_loss = content_weight * content_loss + style_weight * style_loss
            
            # åå‘ä¼ æ’­å’Œä¼˜åŒ–
            optimizer.zero_grad()
            total_loss.backward()
            optimizer.step()
        
        return generated_image.detach()

    def _johnson_style_transfer(self, content_image: torch.Tensor, style_image: torch.Tensor) -> torch.Tensor:
        """
        Johnsonå¿«é€Ÿé£æ ¼è¿ç§»å®ç°ï¼ˆç®€åŒ–ç‰ˆï¼‰
        """
        # Johnsonæ–¹æ³•ä½¿ç”¨é¢„è®­ç»ƒçš„è½¬æ¢ç½‘ç»œç›´æ¥ç”Ÿæˆé£æ ¼åŒ–å›¾åƒ
        # è¿™é‡Œç®€åŒ–å®ç°ä¸ºæ··åˆå†…å®¹å’Œé£æ ¼å›¾åƒ
        
        alpha = 0.8  # é£æ ¼åŒ–ç¨‹åº¦
        stylized_image = alpha * style_image + (1 - alpha) * content_image
        return stylized_image

    def _adain_style_transfer(self, content_image: torch.Tensor, style_image: torch.Tensor, preserve_colors: bool) -> torch.Tensor:
        """
        AdaINé£æ ¼è¿ç§»å®ç°ï¼ˆç®€åŒ–ç‰ˆï¼‰
        """
        if preserve_colors:
            # ä¿ç•™å†…å®¹å›¾åƒçš„é¢œè‰²ä¿¡æ¯
            # è¿™é‡Œç®€åŒ–å®ç°ä¸ºä»…åº”ç”¨é£æ ¼å›¾åƒçš„çº¹ç†ä¿¡æ¯
            stylized_image = content_image.clone()
            
            # æ·»åŠ ä¸€äº›é£æ ¼å™ªå£°
            noise = torch.randn_like(style_image) * 0.1
            stylized_image = stylized_image + noise
            stylized_image = torch.clamp(stylized_image, 0, 1)
        else:
            # æ ‡å‡†AdaINé£æ ¼è¿ç§»
            # è‡ªé€‚åº”å®ä¾‹å½’ä¸€åŒ–
            
            # ç®€åŒ–å®ç°ï¼šæ··åˆå†…å®¹å’Œé£æ ¼
            alpha = 0.7
            stylized_image = alpha * style_image + (1 - alpha) * content_image
            
        return stylized_image

    @classmethod
    def IS_CHANGED(cls, **kwargs):
        return False


class FastStyleTransferNode:
    """
    å¿«é€Ÿé£æ ¼è¿ç§»èŠ‚ç‚¹
    
    ä½¿ç”¨é¢„è®­ç»ƒçš„é£æ ¼è¿ç§»ç½‘ç»œå®ç°å¿«é€Ÿå®æ—¶é£æ ¼è¿ç§»ã€‚
    ç›¸æ¯”äºåŸºäºä¼˜åŒ–çš„æ–¹æ³•ï¼Œé€Ÿåº¦æ›´å¿«ï¼Œé€‚åˆæ‰¹é‡å¤„ç†ã€‚
    """
    
    @classmethod
    def INPUT_TYPES(cls) -> Dict[str, Dict[str, Any]]:
        return {
            "required": {
                "content_image": ("IMAGE",),
                "style_type": ([
                    "candy", "mosaic", "rain_princess", 
                    "udnie", "starry_night", "la_muse"
                ], {"default": "candy"}),
                "model_size": (["small", "medium", "large"], {"default": "medium"})
            },
            "optional": {
                "preserve_content": ("BOOLEAN", {"default": False}),
                "style_intensity": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 2.0, "step": 0.1}),
                "use_gpu": ("BOOLEAN", {"default": True})
            }
        }

    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("stylized_image",)
    FUNCTION = "fast_transfer"
    CATEGORY = "ComfyNN/DeepLearning/ComputerVision"

    def fast_transfer(
        self,
        content_image: torch.Tensor,
        style_type: str,
        model_size: str,
        preserve_content: bool = False,
        style_intensity: float = 1.0,
        use_gpu: bool = True
    ) -> Tuple[torch.Tensor]:
        """
        æ‰§è¡Œå¿«é€Ÿé£æ ¼è¿ç§»
        
        Args:
            content_image: å†…å®¹å›¾åƒ [B, H, W, C]
            style_type: é£æ ¼ç±»å‹
            model_size: æ¨¡å‹å¤§å°
            preserve_content: æ˜¯å¦ä¿ç•™å†…å®¹
            style_intensity: é£æ ¼å¼ºåº¦
            use_gpu: æ˜¯å¦ä½¿ç”¨GPU
            
        Returns:
            stylized_image: é£æ ¼åŒ–å›¾åƒ [B, H, W, C]
        """
        # ç¡®ä¿è¾“å…¥å›¾åƒä¸ºæ­£ç¡®çš„æ ¼å¼
        batch_size, height, width, channels = content_image.shape
        
        # æ¨¡æ‹Ÿå¿«é€Ÿé£æ ¼è¿ç§»è¿‡ç¨‹
        # å®é™…å®ç°ä¸­è¿™é‡Œä¼šåŠ è½½é¢„è®­ç»ƒçš„é£æ ¼è¿ç§»æ¨¡å‹å¹¶è¿›è¡Œæ¨ç†
        
        # æ ¹æ®é£æ ¼ç±»å‹å’Œå¼ºåº¦è°ƒæ•´å›¾åƒ
        if preserve_content:
            # æ›´æ³¨é‡ä¿æŒå†…å®¹ç»“æ„
            alpha = 0.3 * style_intensity
        else:
            # æ ‡å‡†é£æ ¼è¿ç§»
            alpha = 0.7 * style_intensity
        
        # ç”Ÿæˆé£æ ¼åŒ–æ•ˆæœï¼ˆç®€åŒ–å®ç°ï¼‰
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šä½¿ç”¨ç¥ç»ç½‘ç»œç”Ÿæˆè‰ºæœ¯é£æ ¼æ•ˆæœ
        
        # æ·»åŠ é£æ ¼åŒ–å™ªå£°
        noise = torch.randn_like(content_image) * 0.1 * style_intensity
        
        # æ ¹æ®é£æ ¼ç±»å‹è°ƒæ•´é¢œè‰²å€¾å‘
        if style_type == "candy":
            # å¢å¼ºç³–æœèˆ¬çš„é²œè‰³è‰²å½©
            stylized_image = content_image * (1 + 0.2 * style_intensity)
        elif style_type == "starry_night":
            # æ¢µé«˜æ˜Ÿå¤œé£æ ¼ï¼ˆå¢å¼ºç¬”è§¦æ„Ÿï¼‰
            stylized_image = content_image + noise * 2
        elif style_type == "mosaic":
            # é©¬èµ›å…‹è‰ºæœ¯é£æ ¼
            stylized_image = content_image + noise
        else:
            # é»˜è®¤é£æ ¼
            stylized_image = content_image * (1 - alpha) + noise * alpha
        
        # ç¡®ä¿è¾“å‡ºå€¼åœ¨æœ‰æ•ˆèŒƒå›´å†…
        stylized_image = torch.clamp(stylized_image, 0, 1)
        
        return (stylized_image,)

    @classmethod
    def IS_CHANGED(cls, **kwargs):
        return False


# Nodeå¯¼å‡ºæ˜ å°„
NODE_CLASS_MAPPINGS = {
    "StyleTransferNode": StyleTransferNode,
    "FastStyleTransferNode": FastStyleTransferNode
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "StyleTransferNode": "Neural Style Transfer ğŸ±",
    "FastStyleTransferNode": "Fast Style Transfer ğŸ±"
}
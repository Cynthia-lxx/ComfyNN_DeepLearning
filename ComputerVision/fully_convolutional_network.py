import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Tuple, Dict, Any, List

class FCNNode:
    """
    å…¨å·ç§¯ç½‘ç»œ (Fully Convolutional Network) èŠ‚ç‚¹
    
    FCNæ˜¯ç”¨äºè¯­ä¹‰åˆ†å‰²çš„ç»å…¸ç½‘ç»œæ¶æ„ï¼Œå°†ä¼ ç»ŸCNNä¸­çš„å…¨è¿æ¥å±‚æ›¿æ¢ä¸ºå·ç§¯å±‚ï¼Œ
    ä½¿å¾—ç½‘ç»œå¯ä»¥æ¥å—ä»»æ„å°ºå¯¸çš„è¾“å…¥ï¼Œå¹¶äº§ç”Ÿç›¸åº”å°ºå¯¸çš„è¾“å‡ºã€‚
    """
    
    @classmethod
    def INPUT_TYPES(cls) -> Dict[str, Dict[str, Any]]:
        return {
            "required": {
                "image_batch": ("IMAGE",),
                "fcn_variant": (["fcn8s", "fcn16s", "fcn32s"], {"default": "fcn8s"}),
                "num_classes": ("INT", {"default": 10, "min": 2, "max": 1000, "step": 1}),
                "backbone": (["vgg16", "resnet50", "resnet101"], {"default": "vgg16"})
            },
            "optional": {
                "pretrained": ("BOOLEAN", {"default": True}),
                "upsample_method": (["bilinear", "transposed_conv"], {"default": "bilinear"}),
                "output_stride": ("INT", {"default": 32, "min": 8, "max": 64, "step": 8}),
                "use_dropout": ("BOOLEAN", {"default": False}),
                "dropout_rate": ("FLOAT", {"default": 0.5, "min": 0.0, "max": 0.9, "step": 0.05})
            }
        }

    RETURN_TYPES = ("MASK", "TENSOR", "IMAGE")
    RETURN_NAMES = ("segmentation_masks", "class_probabilities", "overlay_image")
    FUNCTION = "fcn_segment"
    CATEGORY = "ComfyNN/DeepLearning/ComputerVision"

    def fcn_segment(
        self,
        image_batch: torch.Tensor,
        fcn_variant: str,
        num_classes: int,
        backbone: str,
        pretrained: bool = True,
        upsample_method: str = "bilinear",
        output_stride: int = 32,
        use_dropout: bool = False,
        dropout_rate: float = 0.5
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        æ‰§è¡ŒFCNè¯­ä¹‰åˆ†å‰²
        
        Args:
            image_batch: è¾“å…¥å›¾åƒæ‰¹æ¬¡ [B, H, W, C]
            fcn_variant: FCNå˜ä½“ (fcn8s, fcn16s, fcn32s)
            num_classes: ç±»åˆ«æ•°é‡
            backbone: éª¨å¹²ç½‘ç»œ
            pretrained: æ˜¯å¦ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
            upsample_method: ä¸Šé‡‡æ ·æ–¹æ³•
            output_stride: è¾“å‡ºæ­¥é•¿
            use_dropout: æ˜¯å¦ä½¿ç”¨dropout
            dropout_rate: Dropoutæ¯”ç‡
            
        Returns:
            segmentation_masks: åˆ†å‰²æ©ç  [B, H, W]
            class_probabilities: ç±»åˆ«æ¦‚ç‡ [B, num_classes, H, W]
            overlay_image: å åŠ å›¾åƒ [B, H, W, C]
        """
        # ç¡®ä¿è¾“å…¥å›¾åƒä¸ºæ­£ç¡®çš„æ ¼å¼
        batch_size, height, width, channels = image_batch.shape
        
        # è½¬æ¢å›¾åƒæ ¼å¼ä¸º [B, C, H, W]
        if channels in [1, 3]:  # ç°åº¦å›¾æˆ–RGBå›¾
            input_tensor = image_batch.permute(0, 3, 1, 2)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å›¾åƒé€šé“æ•°: {channels}")
        
        # æ¨¡æ‹ŸFCNæ¨ç†è¿‡ç¨‹
        # å®é™…å®ç°ä¸­è¿™é‡Œä¼šåŠ è½½é¢„è®­ç»ƒçš„FCNæ¨¡å‹å¹¶è¿›è¡Œæ¨ç†
        
        # ç‰¹å¾æå–ï¼ˆæ¨¡æ‹Ÿï¼‰
        features = torch.rand(batch_size, 512, height // output_stride, width // output_stride)
        
        # åˆ†ç±»å¤´ï¼ˆæ¨¡æ‹Ÿï¼‰
        logits = torch.rand(batch_size, num_classes, height // output_stride, width // output_stride)
        
        # ä¸Šé‡‡æ ·åˆ°åŸå§‹å°ºå¯¸
        if upsample_method == "bilinear":
            upsampled_logits = F.interpolate(
                logits, 
                size=(height, width), 
                mode='bilinear', 
                align_corners=False
            )
        else:  # transposed_conv
            # ä½¿ç”¨è½¬ç½®å·ç§¯ä¸Šé‡‡æ ·
            transposed_conv = nn.ConvTranspose2d(
                in_channels=num_classes,
                out_channels=num_classes,
                kernel_size=output_stride,
                stride=output_stride // 8,  # ç®€åŒ–å¤„ç†
                padding=output_stride // 16
            )
            upsampled_logits = transposed_conv(logits)
            # è°ƒæ•´åˆ°ç›®æ ‡å°ºå¯¸
            upsampled_logits = F.interpolate(
                upsampled_logits,
                size=(height, width),
                mode='bilinear',
                align_corners=False
            )
        
        # åº”ç”¨softmaxè·å–æ¦‚ç‡åˆ†å¸ƒ
        class_probabilities = F.softmax(upsampled_logits, dim=1)
        
        # è·å–åˆ†å‰²æ©ç ï¼ˆæœ€å¤§æ¦‚ç‡ç±»åˆ«ï¼‰
        segmentation_masks = torch.argmax(class_probabilities, dim=1)
        
        # ç”Ÿæˆå åŠ å›¾åƒï¼ˆå°†åˆ†å‰²ç»“æœå åŠ åˆ°åŸå§‹å›¾åƒä¸Šï¼‰
        overlay_image = image_batch.clone()
        
        # æ·»åŠ ä¸€äº›å¯è§†åŒ–æ•ˆæœï¼ˆç®€åŒ–å®ç°ï¼‰
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šæ ¹æ®åˆ†å‰²ç»“æœä¸ºå›¾åƒæ·»åŠ é¢œè‰²ç¼–ç 
        alpha = 0.7
        overlay_image = overlay_image * alpha + (1 - alpha) * overlay_image
        
        return (
            segmentation_masks.long(),  # åˆ†å‰²æ©ç 
            class_probabilities,        # ç±»åˆ«æ¦‚ç‡
            overlay_image               # å åŠ å›¾åƒ
        )

    @classmethod
    def IS_CHANGED(cls, **kwargs):
        return False


class EncoderDecoderNode:
    """
    ç¼–ç å™¨-è§£ç å™¨ç»“æ„èŠ‚ç‚¹
    
    è¿™æ˜¯FCNçš„é€šç”¨æ¶æ„ï¼ŒåŒ…å«ç¼–ç å™¨ï¼ˆä¸‹é‡‡æ ·ï¼‰å’Œè§£ç å™¨ï¼ˆä¸Šé‡‡æ ·ï¼‰éƒ¨åˆ†ï¼Œ
    å¹¿æ³›åº”ç”¨äºè¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸­ã€‚
    """
    
    @classmethod
    def INPUT_TYPES(cls) -> Dict[str, Dict[str, Any]]:
        return {
            "required": {
                "input_tensor": ("TENSOR",),
                "num_classes": ("INT", {"default": 10, "min": 2, "max": 1000, "step": 1}),
                "encoder_depth": ("INT", {"default": 5, "min": 3, "max": 8, "step": 1}),
                "base_channels": ("INT", {"default": 64, "min": 16, "max": 256, "step": 16})
            },
            "optional": {
                "decoder_type": (["upsample", "transposed_conv", "unet_style"], {"default": "upsample"}),
                "use_skip_connections": ("BOOLEAN", {"default": True}),
                "activation": (["relu", "leaky_relu", "elu"], {"default": "relu"}),
                "use_batch_norm": ("BOOLEAN", {"default": True}),
                "dropout_rate": ("FLOAT", {"default": 0.0, "min": 0.0, "max": 0.9, "step": 0.05})
            }
        }

    RETURN_TYPES = ("MASK", "TENSOR", "TENSOR")
    RETURN_NAMES = ("segmentation_masks", "class_probabilities", "feature_map")
    FUNCTION = "encode_decode"
    CATEGORY = "ComfyNN/DeepLearning/ComputerVision"

    def encode_decode(
        self,
        input_tensor: torch.Tensor,
        num_classes: int,
        encoder_depth: int,
        base_channels: int,
        decoder_type: str = "upsample",
        use_skip_connections: bool = True,
        activation: str = "relu",
        use_batch_norm: bool = True,
        dropout_rate: float = 0.0
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        æ‰§è¡Œç¼–ç å™¨-è§£ç å™¨ç»“æ„çš„å‰å‘ä¼ æ’­
        
        Args:
            input_tensor: è¾“å…¥å¼ é‡ [B, C, H, W]
            num_classes: ç±»åˆ«æ•°é‡
            encoder_depth: ç¼–ç å™¨æ·±åº¦
            base_channels: åŸºç¡€é€šé“æ•°
            decoder_type: è§£ç å™¨ç±»å‹
            use_skip_connections: æ˜¯å¦ä½¿ç”¨è·³è·ƒè¿æ¥
            activation: æ¿€æ´»å‡½æ•°ç±»å‹
            use_batch_norm: æ˜¯å¦ä½¿ç”¨æ‰¹å½’ä¸€åŒ–
            dropout_rate: Dropoutæ¯”ç‡
            
        Returns:
            segmentation_masks: åˆ†å‰²æ©ç  [B, H, W]
            class_probabilities: ç±»åˆ«æ¦‚ç‡ [B, num_classes, H, W]
            feature_map: ç‰¹å¾å›¾ [B, C', H, W]
        """
        # ç¡®ä¿è¾“å…¥å¼ é‡ç»´åº¦æ­£ç¡®
        if input_tensor.dim() != 4:
            raise ValueError(f"è¾“å…¥å¼ é‡åº”ä¸º4ç»´ [B, C, H, W]ï¼Œå½“å‰ç»´åº¦: {input_tensor.dim()}")
        
        batch_size, channels, height, width = input_tensor.shape
        
        # ç¼–ç å™¨é˜¶æ®µï¼ˆä¸‹é‡‡æ ·ï¼‰
        encoder_features = []
        x = input_tensor
        
        for i in range(encoder_depth):
            out_channels = base_channels * (2 ** min(i, 4))  # é€æ­¥å¢åŠ é€šé“æ•°
            
            # å·ç§¯å—
            conv1 = nn.Conv2d(
                in_channels=x.shape[1],
                out_channels=out_channels,
                kernel_size=3,
                padding=1
            )
            
            conv2 = nn.Conv2d(
                in_channels=out_channels,
                out_channels=out_channels,
                kernel_size=3,
                padding=1
            )
            
            x = conv1(x)
            if use_batch_norm:
                bn = nn.BatchNorm2d(out_channels)
                x = bn(x)
            
            if activation == "relu":
                x = F.relu(x)
            elif activation == "leaky_relu":
                x = F.leaky_relu(x)
            elif activation == "elu":
                x = F.elu(x)
            
            x = conv2(x)
            if use_batch_norm:
                bn = nn.BatchNorm2d(out_channels)
                x = bn(x)
            
            if activation == "relu":
                x = F.relu(x)
            elif activation == "leaky_relu":
                x = F.leaky_relu(x)
            elif activation == "elu":
                x = F.elu(x)
            
            # ä¿å­˜è·³è·ƒè¿æ¥ç‰¹å¾
            if use_skip_connections:
                encoder_features.append(x)
            
            # ä¸‹é‡‡æ ·ï¼ˆæœ€åä¸€ä¸ªé˜¶æ®µä¸è¿›è¡Œä¸‹é‡‡æ ·ï¼‰
            if i < encoder_depth - 1:
                x = F.max_pool2d(x, kernel_size=2, stride=2)
        
        # è§£ç å™¨é˜¶æ®µï¼ˆä¸Šé‡‡æ ·ï¼‰
        for i in range(encoder_depth - 1):
            target_channels = base_channels * (2 ** min(encoder_depth - 2 - i, 4))
            
            # ä¸Šé‡‡æ ·
            if decoder_type == "upsample":
                x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)
            elif decoder_type == "transposed_conv":
                transposed_conv = nn.ConvTranspose2d(
                    in_channels=x.shape[1],
                    out_channels=target_channels,
                    kernel_size=2,
                    stride=2
                )
                x = transposed_conv(x)
            # unet_style ä¼šç»“åˆè·³è·ƒè¿æ¥
            
            # å¦‚æœä½¿ç”¨è·³è·ƒè¿æ¥ï¼Œåˆå¹¶ç‰¹å¾
            if use_skip_connections and len(encoder_features) > 0:
                skip_feature = encoder_features.pop()
                # è°ƒæ•´å°ºå¯¸ä»¥åŒ¹é…
                if x.shape[2:] != skip_feature.shape[2:]:
                    x = F.interpolate(x, size=skip_feature.shape[2:], mode='bilinear', align_corners=False)
                x = torch.cat([x, skip_feature], dim=1)
            
            # å·ç§¯å—
            conv1 = nn.Conv2d(
                in_channels=x.shape[1],
                out_channels=target_channels,
                kernel_size=3,
                padding=1
            )
            
            conv2 = nn.Conv2d(
                in_channels=target_channels,
                out_channels=target_channels,
                kernel_size=3,
                padding=1
            )
            
            x = conv1(x)
            if use_batch_norm:
                bn = nn.BatchNorm2d(target_channels)
                x = bn(x)
            
            if activation == "relu":
                x = F.relu(x)
            elif activation == "leaky_relu":
                x = F.leaky_relu(x)
            elif activation == "elu":
                x = F.elu(x)
            
            if dropout_rate > 0:
                x = F.dropout(x, p=dropout_rate)
            
            x = conv2(x)
            if use_batch_norm:
                bn = nn.BatchNorm2d(target_channels)
                x = bn(x)
            
            if activation == "relu":
                x = F.relu(x)
            elif activation == "leaky_relu":
                x = F.leaky_relu(x)
            elif activation == "elu":
                x = F.elu(x)
            
            if dropout_rate > 0:
                x = F.dropout(x, p=dropout_rate)
        
        # æœ€ç»ˆåˆ†ç±»å±‚
        final_conv = nn.Conv2d(
            in_channels=x.shape[1],
            out_channels=num_classes,
            kernel_size=1
        )
        logits = final_conv(x)
        
        # ä¸Šé‡‡æ ·åˆ°åŸå§‹è¾“å…¥å°ºå¯¸
        if logits.shape[2:] != (height, width):
            logits = F.interpolate(logits, size=(height, width), mode='bilinear', align_corners=False)
        
        # åº”ç”¨softmaxè·å–æ¦‚ç‡åˆ†å¸ƒ
        class_probabilities = F.softmax(logits, dim=1)
        
        # è·å–åˆ†å‰²æ©ç ï¼ˆæœ€å¤§æ¦‚ç‡ç±»åˆ«ï¼‰
        segmentation_masks = torch.argmax(class_probabilities, dim=1)
        
        return (
            segmentation_masks.long(),  # åˆ†å‰²æ©ç 
            class_probabilities,        # ç±»åˆ«æ¦‚ç‡
            x                           # æœ€åä¸€å±‚ç‰¹å¾å›¾
        )

    @classmethod
    def IS_CHANGED(cls, **kwargs):
        return False


# Nodeå¯¼å‡ºæ˜ å°„
NODE_CLASS_MAPPINGS = {
    "FCNNode": FCNNode,
    "EncoderDecoderNode": EncoderDecoderNode
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "FCNNode": "Fully Convolutional Network ğŸ±",
    "EncoderDecoderNode": "Encoder-Decoder Network ğŸ±"
}
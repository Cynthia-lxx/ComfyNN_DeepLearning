# ComfyNN ComputerVision Fine-tuning Nodes
# Based on d2l-zh implementation (https://github.com/d2l-ai/d2l-zh)
# Thank you d2l-ai team for the excellent educational resource

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models

class FinetuningNode:
    """æ¨¡å‹å¾®è°ƒèŠ‚ç‚¹ï¼ŒåŸºäºd2l-zhå®ç°"""
    
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "pretrained_model_name": (["resnet18", "resnet34", "resnet50", "vgg16", "vgg19"], 
                                        {"default": "resnet18"}),
                "num_classes": ("INT", {"default": 10, "min": 2, "max": 1000}),
                "freeze_backbone": ("BOOLEAN", {"default": False}),
            },
            "optional": {
                "learning_rate_multiplier": ("FLOAT", {"default": 0.1, "min": 0.001, "max": 1.0, "step": 0.001}),
                "dropout_rate": ("FLOAT", {"default": 0.5, "min": 0.0, "max": 0.9, "step": 0.05}),
            }
        }

    RETURN_TYPES = ("CUSTOM", "STRING")
    RETURN_NAMES = ("fine_tuned_model", "finetuning_info")
    FUNCTION = "fine_tune"
    CATEGORY = "ComfyNN/ComputerVision/FineTuning"
    DESCRIPTION = "å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼ŒåŸºäºd2l-zhå®ç°"

    def fine_tune(self, pretrained_model_name, num_classes, freeze_backbone=False, 
                  learning_rate_multiplier=0.1, dropout_rate=0.5):
        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        if pretrained_model_name == "resnet18":
            model = models.resnet18(pretrained=True)
        elif pretrained_model_name == "resnet34":
            model = models.resnet34(pretrained=True)
        elif pretrained_model_name == "resnet50":
            model = models.resnet50(pretrained=True)
        elif pretrained_model_name == "vgg16":
            model = models.vgg16(pretrained=True)
        elif pretrained_model_name == "vgg19":
            model = models.vgg19(pretrained=True)
        else:
            raise ValueError(f"Unsupported model: {pretrained_model_name}")
        
        # å†»ç»“éª¨å¹²ç½‘ç»œå‚æ•°ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if freeze_backbone:
            for param in model.parameters():
                param.requires_grad = False
        
        # ä¿®æ”¹åˆ†ç±»å¤´ä»¥é€‚åº”æ–°çš„ç±»åˆ«æ•°
        if "resnet" in pretrained_model_name:
            # ResNetç³»åˆ—æ¨¡å‹
            in_features = model.fc.in_features
            model.fc = nn.Sequential(
                nn.Dropout(dropout_rate),
                nn.Linear(in_features, num_classes)
            )
            
            # å¦‚æœä¸å†»ç»“éª¨å¹²ç½‘ç»œï¼Œç¡®ä¿åˆ†ç±»å±‚å‚æ•°å¯è®­ç»ƒ
            for param in model.fc.parameters():
                param.requires_grad = True
                
        elif "vgg" in pretrained_model_name:
            # VGGç³»åˆ—æ¨¡å‹
            in_features = model.classifier[6].in_features
            model.classifier[6] = nn.Sequential(
                nn.Dropout(dropout_rate),
                nn.Linear(in_features, num_classes)
            )
            
            # å¦‚æœä¸å†»ç»“éª¨å¹²ç½‘ç»œï¼Œç¡®ä¿åˆ†ç±»å±‚å‚æ•°å¯è®­ç»ƒ
            for param in model.classifier[6].parameters():
                param.requires_grad = True
        
        # ç”Ÿæˆå¾®è°ƒä¿¡æ¯
        finetuning_info = f"Model Fine-tuning Completed\n"
        finetuning_info += f"Base model: {pretrained_model_name}\n"
        finetuning_info += f"Number of classes: {num_classes}\n"
        finetuning_info += f"Backbone frozen: {freeze_backbone}\n"
        finetuning_info += f"Learning rate multiplier: {learning_rate_multiplier}\n"
        finetuning_info += f"Dropout rate: {dropout_rate}\n"
        
        # è®¡ç®—å¯è®­ç»ƒå‚æ•°æ•°é‡
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in model.parameters())
        finetuning_info += f"Trainable parameters: {trainable_params}/{total_params} ({trainable_params/total_params*100:.2f}%)"
        
        return (model, finetuning_info)


class TransferLearningNode:
    """è¿ç§»å­¦ä¹ èŠ‚ç‚¹ï¼ŒåŸºäºd2l-zhå®ç°"""
    
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "source_model": ("CUSTOM",),
                "target_dataset_info": ("STRING", {"default": "CIFAR-10", "multiline": False}),
                "transfer_strategy": (["feature_extractor", "fine_tuning", "adapter"], 
                                    {"default": "feature_extractor"}),
            },
            "optional": {
                "freeze_layers": ("BOOLEAN", {"default": True}),
                "new_layers_dropout": ("FLOAT", {"default": 0.5, "min": 0.0, "max": 0.9, "step": 0.05}),
                "num_target_classes": ("INT", {"default": 10, "min": 2, "max": 1000}),
            }
        }

    RETURN_TYPES = ("CUSTOM", "STRING")
    RETURN_NAMES = ("adapted_model", "transfer_info")
    FUNCTION = "transfer"
    CATEGORY = "ComfyNN/ComputerVision/FineTuning"
    DESCRIPTION = "æ‰§è¡Œè¿ç§»å­¦ä¹ ï¼ŒåŸºäºd2l-zhå®ç°"

    def transfer(self, source_model, target_dataset_info, transfer_strategy, freeze_layers=True, 
                 new_layers_dropout=0.5, num_target_classes=10):
        # æ ¹æ®è¿ç§»ç­–ç•¥è°ƒæ•´æ¨¡å‹
        model = source_model
        
        if transfer_strategy == "feature_extractor":
            # ä½¿ç”¨æºæ¨¡å‹ä½œä¸ºç‰¹å¾æå–å™¨
            if freeze_layers:
                # å†»ç»“ç‰¹å¾æå–å±‚
                for param in model.parameters():
                    param.requires_grad = False
                    
        elif transfer_strategy == "fine_tuning":
            # å¾®è°ƒæ¨¡å‹ï¼ˆéƒ¨åˆ†æˆ–å…¨éƒ¨å±‚ï¼‰
            # è¿™é‡Œæˆ‘ä»¬ç®€å•åœ°ç¡®ä¿æ‰€æœ‰å‚æ•°éƒ½å¯è®­ç»ƒ
            for param in model.parameters():
                param.requires_grad = True
                
        elif transfer_strategy == "adapter":
            # ä½¿ç”¨é€‚é…å™¨æ¨¡å¼
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…å®ç°ä¼šæ›´å¤æ‚
            pass
            
        # ä¿®æ”¹åˆ†ç±»å¤´ä»¥é€‚åº”ç›®æ ‡ä»»åŠ¡
        if hasattr(model, 'fc'):
            # ResNetç³»åˆ—
            in_features = model.fc.in_features
            model.fc = nn.Sequential(
                nn.Dropout(new_layers_dropout),
                nn.Linear(in_features, num_target_classes)
            )
        elif hasattr(model, 'classifier'):
            # VGGç³»åˆ—
            if isinstance(model.classifier, nn.Sequential):
                in_features = model.classifier[-1].in_features
                model.classifier[-1] = nn.Sequential(
                    nn.Dropout(new_layers_dropout),
                    nn.Linear(in_features, num_target_classes)
                )
            else:
                in_features = model.classifier.in_features
                model.classifier = nn.Sequential(
                    nn.Dropout(new_layers_dropout),
                    nn.Linear(in_features, num_target_classes)
                )
        
        # ç¡®ä¿æ–°æ·»åŠ çš„å±‚å‚æ•°å¯è®­ç»ƒ
        if transfer_strategy in ["feature_extractor", "adapter"]:
            if hasattr(model, 'fc'):
                for param in model.fc.parameters():
                    param.requires_grad = True
            elif hasattr(model, 'classifier'):
                if isinstance(model.classifier, nn.Sequential):
                    for param in model.classifier[-1].parameters():
                        param.requires_grad = True
                else:
                    for param in model.classifier.parameters():
                        param.requires_grad = True
        
        # ç”Ÿæˆè¿ç§»å­¦ä¹ ä¿¡æ¯
        transfer_info = f"Transfer Learning Completed\n"
        transfer_info += f"Source model adapted for: {target_dataset_info}\n"
        transfer_info += f"Transfer strategy: {transfer_strategy}\n"
        transfer_info += f"Layers frozen: {freeze_layers}\n"
        transfer_info += f"New layers dropout: {new_layers_dropout}\n"
        transfer_info += f"Target classes: {num_target_classes}\n"
        
        # è®¡ç®—å¯è®­ç»ƒå‚æ•°æ•°é‡
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in model.parameters())
        transfer_info += f"Trainable parameters: {trainable_params}/{total_params} ({trainable_params/total_params*100:.2f}%)"
        
        return (model, transfer_info)

# Node mappings
NODE_CLASS_MAPPINGS = {
    "FinetuningNode": FinetuningNode,
    "TransferLearningNode": TransferLearningNode,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "FinetuningNode": "Finetuning ğŸ±",
    "TransferLearningNode": "Transfer Learning ğŸ±",
}